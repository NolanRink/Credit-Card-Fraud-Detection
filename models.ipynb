{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc50135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, \n",
    "    average_precision_score, accuracy_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dcb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d33302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a320c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, \n",
    "    average_precision_score, confusion_matrix, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2381f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 44\n",
    "CLASS_WEIGHT = 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba100fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory before change: /Users/nolanrink/Desktop/School/Supervised-Learning-Final-Project\n",
      "Changed working directory to: /Users/nolanrink/Desktop/School/Supervised-Learning-Final-Project\n",
      "Top-level files/folders: ['data', 'notebooks']\n"
     ]
    }
   ],
   "source": [
    "# Ensure the notebook runs from project root so relative paths like 'data/creditcard.csv' work\n",
    "import os\n",
    "from pathlib import Path\n",
    "print(\"Working directory before change:\", os.getcwd())\n",
    "project_root = Path('/Users/nolanrink/Desktop/School/Supervised-Learning-Final-Project')\n",
    "if project_root.exists():\n",
    "    os.chdir(project_root)\n",
    "    print(\"Changed working directory to:\", os.getcwd())\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Project root not found: {project_root}\")\n",
    "# Show top-level contents to confirm working directory\n",
    "print('Top-level files/folders:', os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd11faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_x_train = pd.read_csv('data/processed/x_train_smote.csv')\n",
    "smote_y_train = pd.read_csv('data/processed/y_train_smote.csv')\n",
    "\n",
    "x_train = pd.read_csv('data/processed/x_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "\n",
    "x_test = pd.read_csv('data/processed/x_test.csv')\n",
    "y_test = pd.read_csv('data/processed/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecba28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cc723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c88d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688a527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c2b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871a61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        y_pred_proba: Predicted probabilities (for AUC)\n",
    "        model_name: Name of the model (for printing)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'classification_report': classification_report(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} - Test Set Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f} (% of correct predictions)\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f} (% of predicted fraud that are correct)\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f} (% of actual fraud detected)\")\n",
    "    print(f\"F1-Score:  {metrics['f1']:.4f} (harmonic mean of precision & recall)\")\n",
    "    print(f\"ROC-AUC:   {metrics['roc_auc']:.4f} (area under ROC curve)\")\n",
    "    print(f\"PR-AUC:    {metrics['pr_auc']:.4f} (area under precision-recall curve)\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1690256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preprocessing pipelines: scaling, resampling, etc.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "def get_scaler():\n",
    "    \"\"\"\n",
    "    Get a StandardScaler for feature scaling.\n",
    "    \n",
    "    Returns:\n",
    "        StandardScaler: Configured scaler\n",
    "    \"\"\"\n",
    "    return StandardScaler()\n",
    "\n",
    "\n",
    "def create_column_transformer_preprocessor():\n",
    "    \"\"\"\n",
    "    Create a ColumnTransformer that:\n",
    "    - Scales ['Time', 'Amount'] using StandardScaler\n",
    "    - Passes V1-V28 (PCA features) through as-is\n",
    "    \n",
    "    Returns:\n",
    "        ColumnTransformer: Configured transformer\n",
    "    \"\"\"\n",
    "    # Identify PCA features (V1-V28)\n",
    "    pca_features = [f'V{i}' for i in range(1, 29)]\n",
    "    scale_features = ['Time', 'Amount']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scale', StandardScaler(), scale_features),\n",
    "            ('passthrough', 'passthrough', pca_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Create a complete preprocessing pipeline with ColumnTransformer.\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline: Sklearn pipeline with preprocessing\n",
    "    \"\"\"\n",
    "    preprocessor = create_column_transformer_preprocessor()\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('preprocess', preprocessor)\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_model_pipeline(model, with_preprocessing=True):\n",
    "    \"\"\"\n",
    "    Create a full pipeline with preprocessing and model.\n",
    "    \n",
    "    Args:\n",
    "        model: The classifier to use\n",
    "        with_preprocessing (bool): Whether to include preprocessing (default True)\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Complete pipeline\n",
    "    \"\"\"\n",
    "    if with_preprocessing:\n",
    "        return Pipeline([\n",
    "            ('preprocess', create_column_transformer_preprocessor()),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "    else:\n",
    "        return Pipeline([\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "\n",
    "def create_scaling_pipeline():\n",
    "    \"\"\"\n",
    "    Create a simple pipeline for scaling all features.\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline: Sklearn pipeline with scaler\n",
    "    \"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_smote_pipeline():\n",
    "    \"\"\"\n",
    "    Create a pipeline with SMOTE resampling and scaling.\n",
    "    \n",
    "    Returns:\n",
    "        ImbPipeline: Imbalanced-learn pipeline with SMOTE\n",
    "    \"\"\"\n",
    "    return ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_combined_resample_pipeline():\n",
    "    \"\"\"\n",
    "    Create a pipeline combining oversampling (SMOTE) and undersampling.\n",
    "    \n",
    "    Returns:\n",
    "        ImbPipeline: Imbalanced-learn pipeline with combined resampling\n",
    "    \"\"\"\n",
    "    return ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('undersample', RandomUnderSampler(random_state=42)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f385e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        solver='lbfgs')\n",
    "\n",
    "lr_smote = LogisticRegression(\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf85173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training complete!\n",
      "\n",
      "======================================================================\n",
      "Logistic Regression - Test Set Results\n",
      "======================================================================\n",
      "Accuracy:  0.9785 (% of correct predictions)\n",
      "Precision: 0.0668 (% of predicted fraud that are correct)\n",
      "Recall:    0.8878 (% of actual fraud detected)\n",
      "F1-Score:  0.1243 (harmonic mean of precision & recall)\n",
      "ROC-AUC:   0.9704 (area under ROC curve)\n",
      "PR-AUC:    0.7601 (area under precision-recall curve)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55649  1215]\n",
      " [   11    87]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56864\n",
      "           1       0.07      0.89      0.12        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.93      0.56     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression\n",
    "pipe_lr = create_model_pipeline(lr, with_preprocessing=False)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "pipe_lr.fit(x_train, y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = pipe_lr.predict(x_test)\n",
    "y_pred_proba_lr = pipe_lr.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_lr = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b5fa5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training complete!\n",
      "\n",
      "======================================================================\n",
      "Logistic Regression - Test Set Results\n",
      "======================================================================\n",
      "Accuracy:  0.9784 (% of correct predictions)\n",
      "Precision: 0.0666 (% of predicted fraud that are correct)\n",
      "Recall:    0.8878 (% of actual fraud detected)\n",
      "F1-Score:  0.1238 (harmonic mean of precision & recall)\n",
      "ROC-AUC:   0.9709 (area under ROC curve)\n",
      "PR-AUC:    0.7616 (area under precision-recall curve)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55644  1220]\n",
      " [   11    87]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56864\n",
      "           1       0.07      0.89      0.12        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.93      0.56     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression\n",
    "pipe_lr_preprocessed = create_model_pipeline(lr, with_preprocessing=True)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "pipe_lr_preprocessed.fit(x_train, y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = pipe_lr_preprocessed.predict(x_test)\n",
    "y_pred_proba_lr = pipe_lr_preprocessed.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_lr = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47fc5b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training complete!\n",
      "\n",
      "======================================================================\n",
      "Logistic Regression - Test Set Results\n",
      "======================================================================\n",
      "Accuracy:  0.9917 (% of correct predictions)\n",
      "Precision: 0.1557 (% of predicted fraud that are correct)\n",
      "Recall:    0.8673 (% of actual fraud detected)\n",
      "F1-Score:  0.2640 (harmonic mean of precision & recall)\n",
      "ROC-AUC:   0.9741 (area under ROC curve)\n",
      "PR-AUC:    0.7547 (area under precision-recall curve)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56403   461]\n",
      " [   13    85]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.16      0.87      0.26        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.58      0.93      0.63     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression\n",
    "smote_pipe_lr = create_model_pipeline(lr_smote, with_preprocessing=True)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "smote_pipe_lr.fit(smote_x_train, smote_y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = smote_pipe_lr.predict(x_test)\n",
    "y_pred_proba_lr = smote_pipe_lr.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_lr = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191748aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0597053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ed416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77404ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e88b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        solver='lbfgs')\n",
    "\n",
    "lr_smote = LogisticRegression(\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4cf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Compute multiple evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        y_pred_proba: Predicted probabilities (for AUC scores)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf916a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.06682027649769585\n",
      "recall: 0.8877551020408163\n",
      "f1: 0.12428571428571429\n",
      "confusion_matrix: [[55649  1215]\n",
      " [   11    87]]\n",
      "roc_auc: 0.9703988499592296\n",
      "pr_auc: 0.760082195166476\n"
     ]
    }
   ],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "metrics = compute_metrics(y_test, y_pred, lr.predict_proba(x_test)[:, 1])\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28b3f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.15769944341372913\n",
      "recall: 0.8673469387755102\n",
      "f1: 0.2668759811616955\n",
      "confusion_matrix: [[56410   454]\n",
      " [   13    85]]\n",
      "roc_auc: 0.974548654577194\n",
      "pr_auc: 0.755887640870852\n"
     ]
    }
   ],
   "source": [
    "lr_smote.fit(smote_x_train, smote_y_train)\n",
    "y_pred = lr_smote.predict(x_test)\n",
    "metrics = compute_metrics(y_test, y_pred, lr_smote.predict_proba(x_test)[:, 1])\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31576ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
